# Testing Workflows Abstraction - Implementation Plan

**Date**: 2025.10.11
**Status**: In Progress
**Purpose**: Create reusable canonical testing workflows for baseline collection, post-change remediation, and test harness maintenance

---

## Executive Summary

### Problem Statement

Multiple projects (Lupin, COSA) have independently developed comprehensive testing workflows as slash commands. These commands share 80%+ common logic but are duplicated across projects:

- **Current duplication**: ~2000 lines of workflow logic repeated across projects
- **Maintenance burden**: Bug fixes and improvements must be applied to multiple files
- **Inconsistency risk**: Projects diverge in testing practices over time
- **Adoption friction**: New projects must recreate entire testing infrastructure

### Solution Overview

Extract common testing workflow logic into three canonical workflow documents in the planning-is-prompting repository:

1. **testing-baseline.md** - Pre-change baseline collection (pure data gathering)
2. **testing-remediation.md** - Post-change verification and systematic fixes
3. **testing-harness-update.md** - Test maintenance after code changes

Each project creates thin wrapper slash commands (`/plan-test-baseline`, `/plan-test-remediation`, `/plan-test-harness-update`) that configure and invoke these canonical workflows.

### Benefits

**For Existing Projects**:
- Reduce code duplication from ~2000 lines to ~600 canonical + ~100 per project
- Inherit workflow improvements automatically
- Consistent testing practices across all projects
- Easier to maintain and evolve

**For New Projects**:
- Drop-in testing infrastructure via installation wizard
- Just configure paths and prefixes
- Production-ready testing workflows from day one
- Best practices built-in

**For Maintenance**:
- Single source of truth for testing logic
- Fix once, benefit everywhere
- Easy to add new features (scope modes, report formats, etc.)
- Clear upgrade path

---

## Current State Analysis

### Existing Testing Commands in genie-in-the-box

**Lupin Testing Commands** (`.claude/commands/`):
- `smoke-test-baseline.md` (261 lines) - Pre-change baseline with scope parameter (full|lupin)
- `smoke-test-remediation.md` (399 lines) - Post-change verification with remediation modes
- `lupin-test-harness-update.md` (633 lines) - Systematic test update planning after code changes

**COSA Testing Commands** (`src/cosa/.claude/commands/`):
- `smoke-test-baseline.md` (227 lines) - COSA framework baseline (COSA-only variant)
- `smoke-test-remediation.md` (484 lines) - COSA framework remediation (parallel implementation)

**Test Runner Scripts**:
- `src/tests/run-lupin-smoke-tests.sh` (637 lines) - Orchestrates Lupin smoke tests
- `src/cosa/tests/smoke/scripts/run-cosa-smoke-tests.sh` (370 lines) - CoSA smoke tests
- `src/cosa/tests/unit/scripts/run-cosa-unit-tests.sh` (377 lines) - CoSA unit tests

### Common Patterns Identified

**Structural Similarities** (All commands share):
1. **TodoWrite tracking** with `[PREFIX]` tags for progress visibility
2. **Notification integration** (start/progress/complete with priorities)
3. **Health checks** before test execution (server status, imports, etc.)
4. **Timestamped logging** to dedicated directories
5. **Comprehensive markdown reports** with metrics and analysis
6. **Session history updates** at end of workflow
7. **Git safety** (backup/stash before remediation)

**Workflow Phases** (Consistent across commands):
1. Pre-flight validation and setup
2. Test execution with logging
3. Report generation (baseline or comparison)
4. Remediation phase (conditional, scope-based)
5. Validation re-testing
6. History and notification updates

**Configurable Aspects** (Project-specific):
- Working directory and test script paths
- Project prefix for TodoWrite/notifications
- Test scope (full suite vs. selective)
- Remediation scope (FULL|CRITICAL_ONLY|SELECTIVE|ANALYSIS_ONLY)
- Baseline file location and auto-detection
- Framework-specific environment setup (PYTHONPATH, etc.)

### Code Duplication Analysis

**Total lines across all testing commands**: ~2,004 lines
- Lupin commands: 1,293 lines
- COSA commands: 711 lines

**Estimated canonical workflow size**: ~2,400 lines
- testing-baseline.md: ~800 lines (parameterized logic)
- testing-remediation.md: ~1,000 lines (comparison + remediation)
- testing-harness-update.md: ~600 lines (change analysis + planning)

**Estimated thin wrapper size**: ~50-80 lines per project
- Configuration section: 20-30 lines
- Instructions to Claude: 20-30 lines
- Argument handling: 10-20 lines

**Net reduction**: From ~2,000 duplicated lines to ~600 canonical + ~200 wrappers = **70% reduction in maintenance burden**

---

## Target Architecture

### Repository Structure

**planning-is-prompting/workflow/**:
```
testing-baseline.md           # Canonical baseline workflow (~800 lines)
testing-remediation.md        # Canonical remediation workflow (~1000 lines)
testing-harness-update.md    # Canonical test update workflow (~600 lines)
INSTALLATION-GUIDE.md         # Updated with testing workflow examples
```

**planning-is-prompting/.claude/commands/**:
```
plan-test-baseline.md         # Dogfooding wrapper (configured for this repo)
plan-test-remediation.md      # Dogfooding wrapper (configured for this repo)
plan-test-harness-update.md  # Dogfooding wrapper (configured for this repo)
```

**Any project** (e.g., genie-in-the-box)/.claude/commands/:
```
plan-test-baseline.md         # Thin wrapper with project-specific config
plan-test-remediation.md      # Thin wrapper with project-specific config
plan-test-harness-update.md  # Thin wrapper with project-specific config
```

### Naming Convention

**All commands use `plan-` prefix**:
- Indicates source repository: planning-is-prompting
- Shows that command references canonical workflow
- Project-agnostic naming (same command name across all projects)

**Examples**:
- `/plan-test-baseline` - Works in any project, configured via wrapper
- `/plan-test-remediation` - Works in any project, configured via wrapper
- `/plan-test-harness-update` - Works in any project, configured via wrapper

### Thin Wrapper Pattern

**Each wrapper contains**:
1. **Frontmatter** (YAML):
   - Description
   - Allowed tools
   - Arguments (if any)

2. **Project Configuration Section**:
   - Project prefix (`[LUPIN]`, `[COSA]`, `[MYPROJECT]`)
   - Working directory paths
   - Test script locations
   - Report/log directories
   - Test scope options
   - Environment variables

3. **Instructions to Claude**:
   - Read canonical workflow from planning-is-prompting
   - Execute with above configuration
   - Pass arguments to workflow

**No logic duplication** - All workflow steps live in canonical documents.

---

## Configuration Schema

### Project Configuration Parameters

Each thin wrapper must provide these configuration values:

```yaml
project_config:
  # Identity
  short_prefix: "[PROJECT]"           # Used in TodoWrite items and notifications
  project_name: "Project Name"        # Human-readable name

  # Paths
  working_directory: "/path/to/project"
  logs_directory: "tests/results/logs"     # Standardized location
  reports_directory: "tests/results/reports"  # Standardized location

  # Test Types and Scripts
  test_types:
    - smoke                           # Always included
    - unit                            # Optional (most projects)
    - integration                     # Optional (mature projects)

  smoke_test_script: "./tests/run-smoke-tests.sh"
  unit_test_script: "./tests/run-unit-tests.sh"    # Optional
  integration_test_script: "./tests/run-integration-tests.sh"  # Optional

  # Baseline Management
  baseline_pattern: "*baseline-smoke-test-report.md"
  baseline_auto_detect: true

  # Health Checks
  health_check_url: "http://localhost:PORT/health"  # Optional
  health_check_command: "command to verify system"   # Alternative

  # Environment Setup
  pythonpath: "/path/to/src"          # Optional
  environment_vars:                   # Optional
    - "VAR_NAME=value"

  # Scope Options (for baseline/remediation)
  scope_options:
    - full: "Run all test suites"
    - project_only: "Run project tests only (skip submodules)"
    - quick: "Run critical tests only"
```

### Scope Parameter Handling

**Baseline** (`/plan-test-baseline [scope]`):
- `full` (default) - All test suites including submodules
- `project_only` - Project tests only, skip dependencies
- `quick` - Critical smoke tests only

**Remediation** (`/plan-test-remediation [baseline_report] [remediation_scope]`):
- Auto-detect latest baseline if not specified
- Remediation scopes:
  - `FULL` (default) - Fix all issues in priority order
  - `CRITICAL_ONLY` - Fix only blocking issues
  - `SELECTIVE` - Interactive issue selection
  - `ANALYSIS_ONLY` - Generate report without fixes

**Harness Update** (`/plan-test-harness-update [date_range]`):
- Auto-detect recent changes if date not specified
- Analyzes code changes and proposes test updates

---

## Canonical Workflow Specifications

### 1. testing-baseline.md (~800 lines)

**Purpose**: Establish comprehensive baseline before major changes (pure data collection, zero remediation)

**Key Sections**:

#### Header (50 lines)
- Purpose statement
- When to use this workflow
- Key principles (Observe First, Fix Later)
- Scope options documentation
- Usage examples

#### Step 0: Initialize TodoWrite (80 lines)
- Create progress tracking checklist
- Include [PREFIX] tags from config
- Scope-specific task lists
- Timestamp capture

#### Step 1: Notification - Start (40 lines)
- Send start notification with scope
- Priority: medium
- Format: `[PREFIX] üîç Baseline collection STARTED (SCOPE) - message`

#### Step 2: Setup Environment (100 lines)
- Create log/report directories from config
- Check health endpoint or command from config
- Verify test scripts exist and are executable
- Set environment variables from config
- Generate unique timestamp for logs

#### Step 3: Execute Test Suites (150 lines)
- **For each test type** in config (smoke, unit, integration):
  - Execute test script with logging
  - Capture full output to timestamped log
  - Extract basic metrics (tests run, failures, pass rate)
  - Handle script errors gracefully
- **Scope handling**:
  - `full`: Run all configured test types
  - `project_only`: Skip dependency/submodule tests
  - `quick`: Run smoke tests only with --quick flag

#### Step 4: Generate Baseline Report (200 lines)
- **Report file**: `{reports_directory}/YYYY.MM.DD-baseline-test-report.md`
- **Sections**:
  - Executive Summary (overall health, pass rates)
  - Per-suite results (smoke, unit, integration)
  - Category breakdowns with pass/fail counts
  - Failed tests by priority (CRITICAL/HIGH/MEDIUM)
  - Performance metrics (execution time, response times)
  - Baseline timestamp for future comparison
- **Templates** for each test type
- **Conditional sections** based on scope

#### Step 5: Update History (80 lines)
- Add baseline collection entry to history.md
- Include summary metrics
- Link to report file
- Note scope used

#### Step 6: Notification - Complete (40 lines)
- Send completion notification
- Include pass rate and health status
- Priority: medium

#### Step 7: Final TodoWrite Update (40 lines)
- Mark all tasks as completed
- Provide summary metrics

#### Critical Reminders Section (20 lines)
- ‚ùå DO NOT: Fix issues, modify code, restart services
- ‚úÖ DO: Comprehensive logging, accurate metrics, pattern recognition

**Parameterization Strategy**:
- Use `{config.parameter}` syntax for all project-specific values
- Conditional sections based on test_types array
- Scope-based branching logic
- Template sections that adapt to configuration

---

### 2. testing-remediation.md (~1000 lines)

**Purpose**: Verify system health after changes, identify regressions, systematically remediate issues

**Key Sections**:

#### Header (60 lines)
- Purpose statement
- When to use this workflow
- Argument documentation (baseline_report, remediation_scope)
- Key principles (Compare, Analyze, Fix, Validate)

#### Step 0: Pre-Flight Validation (120 lines)
- Auto-detect baseline report or validate provided path
- Parse remediation scope argument
- Create backup/stash of current git state
- Validate remediation scope parameter
- Verify baseline report is readable
- Set up session directories

#### Step 1: Initialize TodoWrite (100 lines)
- Create comprehensive task list
- Include remediation scope in tasks
- Phase-based organization:
  - Phase 1: Critical regressions
  - Phase 2: High priority issues
  - Phase 3: Medium priority issues
- Validation and reporting tasks

#### Step 2: Notification - Start (40 lines)
- Send start notification with scope
- Include baseline reference
- Priority: medium

#### Step 3: Execute Post-Change Tests (150 lines)
- Same logic as baseline Step 3
- Execute all test suites from config
- Capture to different log file naming pattern
- Extract current metrics

#### Step 4: Baseline Comparison Analysis (250 lines)
- **Analysis file**: `{reports_directory}/YYYY.MM.DD-comparison-analysis.md`
- **Comparison logic**:
  - Read baseline report metrics
  - Compare current vs baseline pass rates
  - Identify regressions (PASS‚ÜíFAIL)
  - Identify improvements (FAIL‚ÜíPASS)
  - Calculate performance deltas
- **Regression categorization**:
  - **CRITICAL**: Core functionality broken, imports failed, 0% pass rate
  - **HIGH**: Major features broken, >20% performance degradation
  - **MEDIUM**: Edge cases, minor features, <20% degradation
- **Generate remediation plan** based on scope:
  - FULL: All phases in priority order
  - CRITICAL_ONLY: Phase 1 only
  - SELECTIVE: Present options to user
  - ANALYSIS_ONLY: Report generation only

#### Step 5: Systematic Remediation (300 lines)
- **Scope-based execution**:
  - ANALYSIS_ONLY: Skip to Step 7
  - CRITICAL_ONLY: Execute Phase 1 only
  - SELECTIVE: Prompt user for issue selection
  - FULL: Execute all phases

- **Phase 1: Critical Issues** (10 min per issue max):
  - For each critical regression:
    1. Analyze specific failure
    2. Identify root cause in changes
    3. Implement targeted fix
    4. Test the specific fix
    5. Update progress tracker
  - Time-box each issue
  - Document unsolved issues

- **Phase 2: High Priority Issues** (5 min per issue max):
  - Same process as Phase 1
  - Tighter time constraints
  - Focus on major functionality

- **Phase 3: Medium Priority Issues** (2 min per issue max):
  - Quick fixes only
  - Defer complex issues
  - Document remaining work

- **Progress tracking table**:
  - Issue ID, category, priority, description
  - Status, fix applied, time spent, validated
  - Real-time updates

#### Step 6: Validation Re-Testing (100 lines)
- Run targeted tests for fixed categories
- Compare results to ensure fixes work
- Verify no new regressions introduced
- Final full test suite execution

#### Step 7: Final Results Documentation (150 lines)
- **Final report**: `{reports_directory}/YYYY.MM.DD-final-remediation-report.md`
- **Sections**:
  - Session metrics (duration, issues found/fixed)
  - Performance impact analysis (baseline‚Üípre-remediation‚Üípost-remediation)
  - Successfully fixed issues
  - Changes made to codebase
  - Remaining issues with justification
  - Rollback instructions if needed
  - Framework status assessment

#### Step 8: Update History (80 lines)
- Add remediation session to history.md
- Include comparison metrics
- Document fixes applied
- Link to reports

#### Step 9: Notification - Complete (40 lines)
- Send completion notification
- Include final pass rate and issues fixed
- Priority based on outcome (urgent if degraded)

#### Step 10: Final TodoWrite Update (50 lines)
- Mark tasks complete
- Provide detailed summary

#### Remediation Guidelines Section (50 lines)
- Time management rules
- Fix prioritization logic
- Emergency escalation triggers
- Success criteria

**Parameterization Strategy**:
- Baseline report path resolution
- Scope-based branching throughout
- Time limits configurable per priority
- Test script invocation from config

---

### 3. testing-harness-update.md (~600 lines)

**Purpose**: Systematic prompt for updating test harnesses after code changes

**Key Sections**:

#### Header (50 lines)
- Purpose statement
- When to use (after significant code changes)
- Target: Claude Code for automated test analysis
- Key capabilities overview

#### Phase 1: Automated Discovery (150 lines)
- **Repository Context Discovery**:
  - Git log analysis for changed files
  - Submodule change detection (if applicable)
  - File categorization by directory
  - New vs modified vs deleted files

- **Component Type Classification**:
  - Map file paths to component types
  - Determine testing requirements per type
  - Identify test locations for each component

- **Current Test Coverage Inventory**:
  - Count existing unit tests by category
  - Find smoke test functions (quick_smoke_test)
  - Identify integration test coverage
  - Gap analysis

#### Phase 2: Test Reconciliation (100 lines)
- **Historical Analysis**:
  - Distinction between smoke/unit/integration
  - Coverage pattern identification
  - Infrastructure strength assessment
  - Integration point discovery

- **Reconciliation Strategy**:
  - Prioritize by impact (CRITICAL‚ÜíHIGH‚ÜíMEDIUM‚ÜíLOW)
  - Apply test type logic (new vs modified components)
  - Leverage existing infrastructure
  - Cross-repository considerations

#### Phase 3: Systematic Update Framework (120 lines)
- **Change Impact Analysis**:
  - Map changes to test requirements
  - Identify existing tests needing updates
  - Determine new tests needed
  - Calculate priority levels

- **Gap Identification**:
  - Missing unit tests
  - Missing smoke tests
  - Outdated tests
  - Missing integration coverage
  - Cross-repo impact

- **Priority-Based Task Generation**:
  - CRITICAL (same day): Core functionality, security, data integrity
  - HIGH (this week): New components, API changes, config mods
  - MEDIUM (next sprint): Integration points, performance, docs
  - LOW (future): Refactoring, cleanup, minor enhancements

#### Phase 4: Implementation Guidance (150 lines)
- **Test Creation Templates**:
  - Unit test template (with MockManager)
  - Smoke test enhancement template
  - Integration test template

- **Test Naming Conventions**:
  - File naming standards
  - Test method naming patterns
  - Focus on recent changes

#### Phase 5: Validation & Quality Assurance (80 lines)
- **Coverage Requirements**:
  - Critical components: 95% line, 100% method, 90% branch
  - Standard components: 85% line, 95% method, 80% branch
  - Support components: 75% line, 90% method, 70% branch

- **Success Metrics**:
  - Validation checklist
  - Quality gates
  - Continuous monitoring commands

#### Usage Instructions (50 lines)
- Quick start guide
- Full implementation workflow
- Example execution context
- Sample prompts for Claude Code

**Parameterization Strategy**:
- Git log commands with project paths
- Test directory paths from config
- Component classification rules (project-specific)
- Coverage thresholds (configurable)

---

## Implementation Phases

### Phase 1: Create Planning Document ‚úÖ

**File**: `src/rnd/2025.10.11-testing-workflows-abstraction.md` (this document)

**Contents**: Complete implementation blueprint with specifications

**Estimated time**: 10 minutes

**Status**: **IN PROGRESS**

---

### Phase 2: Create Canonical Workflows (3-4 hours)

**Task 2.1: Create testing-baseline.md** (1.5 hours)
- Extract common patterns from existing baseline commands
- Implement parameterized configuration system
- Write all 7 workflow steps with conditional logic
- Add comprehensive documentation and examples
- Include critical reminders and success criteria
- **File location**: `workflow/testing-baseline.md`
- **Target size**: ~800 lines

**Task 2.2: Create testing-remediation.md** (2 hours)
- Extract common patterns from existing remediation commands
- Implement baseline comparison logic
- Create priority-based remediation framework
- Add scope-based branching (FULL|CRITICAL_ONLY|SELECTIVE|ANALYSIS_ONLY)
- Include validation and rollback procedures
- Write comprehensive guidelines section
- **File location**: `workflow/testing-remediation.md`
- **Target size**: ~1000 lines

**Task 2.3: Create testing-harness-update.md** (1 hour)
- Extract patterns from lupin-test-harness-update.md
- Implement git-based change discovery
- Create test gap analysis framework
- Add test creation templates
- Include usage examples
- **File location**: `workflow/testing-harness-update.md`
- **Target size**: ~600 lines

---

### Phase 3: Create Dogfooding Wrappers (30 minutes)

**Purpose**: Test canonical workflows in planning-is-prompting itself

**Task 3.1: Create plan-test-baseline.md** (10 minutes)
- **File location**: `.claude/commands/plan-test-baseline.md`
- **Configuration**:
  - Prefix: `[PLAN]`
  - Working directory: `/mnt/DATA01/.../planning-is-prompting`
  - Test types: smoke (this is a docs-only repo)
  - Smoke script: Simple validation script (to be created)
  - Logs directory: `tests/results/logs`
  - Reports directory: `tests/results/reports`
- **Target size**: ~60 lines

**Task 3.2: Create plan-test-remediation.md** (10 minutes)
- **File location**: `.claude/commands/plan-test-remediation.md`
- **Configuration**: Same as above
- **Arguments**: baseline_report (optional), scope (default: ANALYSIS_ONLY for docs repo)
- **Target size**: ~70 lines

**Task 3.3: Create plan-test-harness-update.md** (10 minutes)
- **File location**: `.claude/commands/plan-test-harness-update.md`
- **Configuration**: Same as above
- **Arguments**: date_range (optional)
- **Target size**: ~60 lines

**Note**: For planning-is-prompting, tests are minimal (docs validation only), but wrappers demonstrate the pattern.

---

### Phase 4: Update Documentation (30 minutes)

**Task 4.1: Update INSTALLATION-GUIDE.md** (15 minutes)
- **File**: `workflow/INSTALLATION-GUIDE.md`
- **Add section**: "Testing Workflows" (~150 lines)
- **Contents**:
  - Overview of three testing workflows
  - Configuration parameter reference
  - Example wrapper for simple project
  - Example wrapper for complex project (multi-suite)
  - Scope parameter documentation
  - Integration with installation wizard

**Task 4.2: Update README.md** (5 minutes)
- **File**: `README.md`
- **Changes**:
  - Add testing workflows to feature list
  - Add quick reference to testing commands
  - Link to INSTALLATION-GUIDE.md testing section

**Task 4.3: Update CLAUDE.md** (10 minutes)
- **File**: `CLAUDE.md`
- **Changes**:
  - Add testing workflows to repository structure diagram
  - Add dogfooding example (how this repo uses its own workflows)
  - Update slash command list

---

### Phase 5: Testing & Validation (1 hour)

**Task 5.1: Test plan-test-baseline** (20 minutes)
- Execute `/plan-test-baseline` in planning-is-prompting
- Verify TodoWrite tracking works
- Check notification integration
- Validate report generation
- Confirm log file creation

**Task 5.2: Test plan-test-remediation** (20 minutes)
- Execute `/plan-test-remediation` with ANALYSIS_ONLY scope
- Verify baseline comparison logic
- Check report generation
- Validate scope parameter handling

**Task 5.3: Test plan-test-harness-update** (20 minutes)
- Execute `/plan-test-harness-update` for recent workflow changes
- Verify git change discovery
- Check gap analysis output
- Validate recommendations

**Task 5.4: Documentation review** (Additional)
- Verify all examples are accurate
- Check all cross-references work
- Validate installation guide completeness

---

## Example Thin Wrappers

### Example 1: Simple Project (Single Test Suite)

**File**: `myproject/.claude/commands/plan-test-baseline.md`

```markdown
---
description: Run baseline tests for MyProject (pre-change data collection)
allowed-tools: Bash(.*), TodoWrite, Read, Write, Edit
---

# Baseline Testing for MyProject

**Project Configuration:**
- **Prefix**: [MYPROJECT]
- **Working Directory**: /path/to/myproject
- **Test Types**: smoke
- **Smoke Script**: ./tests/run-tests.sh
- **Logs Directory**: tests/results/logs
- **Reports Directory**: tests/results/reports
- **Baseline Pattern**: *baseline-test-report.md

**Health Check**: curl http://localhost:8000/health

---

## Instructions to Claude

1. **Read the canonical workflow**: planning-is-prompting ‚Üí workflow/testing-baseline.md

2. **Execute the baseline collection workflow** using the configuration above.

3. **Scope**: This project has a single test suite (smoke tests only).

That's it - the canonical workflow handles all logic.
```

---

### Example 2: Complex Project (Multi-Suite with Scopes)

**File**: `genie-in-the-box/.claude/commands/plan-test-baseline.md`

```markdown
---
description: Run baseline tests for Genie-in-the-Box (pre-change data collection)
allowed-tools: Bash(.*), TodoWrite, Read, Write, Edit
arguments:
  - name: scope
    description: Test scope (full|lupin|cosa)
    required: false
    default: full
---

# Baseline Testing for Genie-in-the-Box

**Project Configuration:**
- **Lupin Prefix**: [LUPIN]
- **COSA Prefix**: [COSA]
- **Working Directory**: /mnt/DATA01/include/www.deepily.ai/projects/genie-in-the-box
- **Scope**: ${1:-full}

**Scope Definitions**:
- `full`: Run both Lupin and COSA test suites (comprehensive)
- `lupin`: Run only Lupin tests (FastAPI, WebSocket, etc.)
- `cosa`: Run only COSA framework tests (agents, memory, rest)

**Test Configuration by Scope**:

**If scope = full or lupin**:
- Prefix: [LUPIN]
- Test Types: smoke, unit, integration
- Smoke Script: ./src/tests/run-lupin-smoke-tests.sh
- Unit Script: ./src/tests/run-lupin-unit-tests.sh
- Integration Script: ./src/tests/run-integration-tests.sh
- Logs Directory: src/tests/logs
- Reports Directory: src/tests/results/reports
- Health Check: curl http://localhost:7999/health

**If scope = full or cosa**:
- Prefix: [COSA]
- Test Types: smoke, unit
- Smoke Script: ./src/cosa/tests/smoke/scripts/run-cosa-smoke-tests.sh
- Unit Script: ./src/cosa/tests/unit/scripts/run-cosa-unit-tests.sh
- Logs Directory: src/cosa/tests/results/logs
- Reports Directory: src/cosa/tests/results/reports
- PYTHONPATH: /mnt/DATA01/include/www.deepily.ai/projects/genie-in-the-box/src

---

## Instructions to Claude

1. **Read the canonical workflow**: planning-is-prompting ‚Üí workflow/testing-baseline.md

2. **Execute the baseline collection workflow** based on scope:
   - If scope = "full": Execute both Lupin and COSA configurations sequentially
   - If scope = "lupin": Execute only Lupin configuration
   - If scope = "cosa": Execute only COSA configuration

3. **Combine results**: If scope = "full", generate a unified report showing both test suites.

The canonical workflow handles all execution logic - this wrapper just provides the routing.
```

---

### Example 3: Remediation Wrapper

**File**: `genie-in-the-box/.claude/commands/plan-test-remediation.md`

```markdown
---
description: Run post-change verification and remediation
allowed-tools: Bash(.*), TodoWrite, Read, Write, Edit, Grep, Glob
arguments:
  - name: baseline_report
    description: Path to baseline report (auto-detects if not provided)
    required: false
  - name: scope
    description: Remediation scope (FULL|CRITICAL_ONLY|SELECTIVE|ANALYSIS_ONLY)
    required: false
    default: FULL
---

# Post-Change Remediation for Genie-in-the-Box

**Project Configuration:**
- **Prefix**: [LUPIN]
- **Working Directory**: /mnt/DATA01/include/www.deepily.ai/projects/genie-in-the-box
- **Baseline Report**: ${1:-auto}
- **Remediation Scope**: ${2:-FULL}

**Baseline Auto-Detection**:
- Pattern: `src/tests/results/reports/*baseline-test-report.md`
- Sort by: Most recent timestamp
- Fallback: Prompt user if none found

**Test Configuration**: (Same as plan-test-baseline.md)
- Test Types: smoke, unit, integration
- Smoke Script: ./src/tests/run-lupin-smoke-tests.sh
- Logs Directory: src/tests/logs
- Reports Directory: src/tests/results/reports

**Remediation Scope Options**:
- `FULL` (default): Fix all issues in priority order (Critical‚ÜíHigh‚ÜíMedium)
- `CRITICAL_ONLY`: Fix only blocking issues, document the rest
- `SELECTIVE`: Interactive - present issues for user selection
- `ANALYSIS_ONLY`: Generate comparison report, no fixes

---

## Instructions to Claude

1. **Read the canonical workflow**: planning-is-prompting ‚Üí workflow/testing-remediation.md

2. **Execute the post-change verification and remediation workflow** using the configuration above.

3. **Baseline handling**:
   - If ${1} = "auto" or empty: Auto-detect latest baseline in reports directory
   - Otherwise: Use provided baseline path

4. **Apply remediation scope** from ${2} parameter.

The canonical workflow handles baseline comparison, regression identification, and systematic remediation.
```

---

## Migration Strategy

### Coexistence Approach

**Keep existing ad-hoc commands untouched**:
- `smoke-test-baseline.md` (Lupin)
- `smoke-test-remediation.md` (Lupin)
- `lupin-test-harness-update.md`
- `src/cosa/.claude/commands/smoke-test-baseline.md`
- `src/cosa/.claude/commands/smoke-test-remediation.md`

**Add new canonical-based commands alongside**:
- `.claude/commands/plan-test-baseline.md` (new)
- `.claude/commands/plan-test-remediation.md` (new)
- `.claude/commands/plan-test-harness-update.md` (new)

### Transition Plan

**Phase 1: Parallel Operation** (Current phase)
- Both old and new commands exist
- Test new commands thoroughly
- Validate feature parity
- Gather feedback on improvements

**Phase 2: Deprecation Notice** (After validation)
- Add deprecation notice to old commands
- Point users to new commands
- Document differences (if any)

**Phase 3: Removal** (User-initiated)
- User manually removes old commands when ready
- No forced migration
- Old commands remain as reference implementations

### Benefits of Coexistence

1. **Zero risk**: Old commands continue working
2. **Easy comparison**: Can test both approaches side-by-side
3. **Gradual adoption**: Switch when comfortable
4. **Rollback option**: Can revert to old commands if issues arise
5. **Learning period**: Understand new patterns before full commitment

---

## Testing & Validation

### Validation Approach

**1. Unit-Level Validation** (Each canonical workflow):
- Verify all configuration parameters are used correctly
- Check conditional branching logic (scope, test types)
- Validate report template generation
- Test error handling and edge cases

**2. Integration Validation** (Thin wrapper + canonical workflow):
- Execute `/plan-test-baseline` in planning-is-prompting
- Verify configuration is passed correctly
- Check TodoWrite tracking works
- Validate notification integration
- Confirm report generation

**3. Feature Parity Validation** (Old vs New):
- Compare output reports (should be equivalent)
- Verify all features from old commands exist in new
- Check for any missing functionality
- Validate scope parameter behavior matches

**4. Cross-Project Validation** (After genie-in-the-box adoption):
- Test same canonical workflows with different configurations
- Verify project-specific settings work correctly
- Check multi-suite handling (Lupin + COSA)
- Validate scope-based execution

### Test Cases

**Baseline Collection** (`/plan-test-baseline`):
- ‚úÖ Execute with default scope (full)
- ‚úÖ Execute with project-only scope
- ‚úÖ Execute with quick scope
- ‚úÖ Verify log files created in correct directory
- ‚úÖ Verify report generated with correct metrics
- ‚úÖ Check TodoWrite tracking throughout
- ‚úÖ Validate notification integration (start + complete)
- ‚úÖ Confirm history.md updated

**Remediation** (`/plan-test-remediation`):
- ‚úÖ Auto-detect latest baseline
- ‚úÖ Use specified baseline path
- ‚úÖ Execute with ANALYSIS_ONLY scope
- ‚úÖ Execute with CRITICAL_ONLY scope
- ‚úÖ Execute with FULL scope
- ‚úÖ Verify baseline comparison logic
- ‚úÖ Check regression identification
- ‚úÖ Validate remediation phase execution
- ‚úÖ Confirm re-testing after fixes
- ‚úÖ Verify final report includes all sections

**Harness Update** (`/plan-test-harness-update`):
- ‚úÖ Auto-detect recent changes
- ‚úÖ Use specified date range
- ‚úÖ Verify git change discovery
- ‚úÖ Check component classification
- ‚úÖ Validate gap analysis
- ‚úÖ Confirm priority-based recommendations
- ‚úÖ Verify test templates included

### Success Criteria

**Functional Requirements**:
- ‚úÖ All three canonical workflows execute successfully
- ‚úÖ Thin wrappers correctly configure and invoke workflows
- ‚úÖ All configuration parameters work as documented
- ‚úÖ Feature parity with existing ad-hoc commands achieved
- ‚úÖ Reports generated in standardized format
- ‚úÖ TodoWrite tracking works throughout
- ‚úÖ Notification integration functional

**Quality Requirements**:
- ‚úÖ Workflows are well-documented and clear
- ‚úÖ Configuration schema is intuitive
- ‚úÖ Error messages are helpful
- ‚úÖ Examples are accurate and complete
- ‚úÖ Installation guide is comprehensive
- ‚úÖ Code is maintainable and extensible

**Performance Requirements**:
- ‚úÖ No significant overhead vs ad-hoc commands
- ‚úÖ Log files and reports generated efficiently
- ‚úÖ Test execution time comparable to direct script invocation

---

## Success Criteria

### Phase 1: Planning ‚úÖ
- ‚úÖ Planning document created and comprehensive
- ‚úÖ All specifications detailed and actionable
- ‚úÖ Architecture clearly defined
- ‚úÖ Examples provided for reference

### Phase 2: Implementation
- ‚úÖ Three canonical workflows created (~2,400 total lines)
- ‚úÖ All configuration parameters implemented
- ‚úÖ Conditional logic working correctly
- ‚úÖ Report templates functional
- ‚úÖ Error handling comprehensive

### Phase 3: Dogfooding
- ‚úÖ Three thin wrapper commands created
- ‚úÖ Commands work in planning-is-prompting
- ‚úÖ Configuration correctly applied
- ‚úÖ Workflows execute successfully

### Phase 4: Documentation
- ‚úÖ INSTALLATION-GUIDE.md updated with testing section
- ‚úÖ README.md updated with feature list
- ‚úÖ CLAUDE.md updated with repository structure
- ‚úÖ All examples accurate and tested

### Phase 5: Validation
- ‚úÖ All test cases passed
- ‚úÖ Feature parity confirmed
- ‚úÖ No regressions introduced
- ‚úÖ Performance acceptable
- ‚úÖ Documentation complete and accurate

### Overall Success
- ‚úÖ Canonical workflows are reusable across projects
- ‚úÖ Thin wrapper pattern proven effective
- ‚úÖ Code duplication reduced by ~70%
- ‚úÖ Maintenance burden significantly decreased
- ‚úÖ Installation guide enables easy adoption
- ‚úÖ Existing ad-hoc commands remain untouched (coexistence)
- ‚úÖ No disruption to current workflows

---

## Estimated Timeline

**Total time**: ~6 hours

| Phase | Tasks | Estimated Time | Status |
|-------|-------|----------------|--------|
| Phase 1 | Planning document | 10 minutes | **IN PROGRESS** |
| Phase 2 | Canonical workflows (3 files) | 3-4 hours | Pending |
| Phase 3 | Dogfooding wrappers (3 files) | 30 minutes | Pending |
| Phase 4 | Documentation updates (3 files) | 30 minutes | Pending |
| Phase 5 | Testing & validation | 1 hour | Pending |

---

## Future Enhancements

**Potential improvements** (beyond initial implementation):

1. **Installation Wizard Integration**:
   - Add testing workflows to `/plan-install-wizard`
   - Auto-generate thin wrappers based on user config
   - Detect existing test scripts automatically

2. **Advanced Reporting**:
   - HTML report generation option
   - Trend analysis across multiple baselines
   - Visualization of test coverage over time

3. **CI/CD Integration**:
   - GitHub Actions workflow templates
   - GitLab CI/CD configuration examples
   - Automated baseline saving on main branch

4. **Performance Tracking**:
   - Historical performance database
   - Automated performance regression detection
   - Resource usage tracking (memory, CPU)

5. **Multi-Project Aggregation**:
   - Cross-project test result dashboard
   - Shared baseline comparison
   - Organization-wide testing standards

6. **Smart Remediation**:
   - AI-powered root cause suggestions
   - Automated fix generation for common issues
   - Learning from past remediation sessions

---

## Conclusion

This implementation plan provides a clear path to abstract testing workflows into reusable canonical documents, reducing code duplication by ~70% while maintaining feature parity with existing ad-hoc commands.

The thin wrapper pattern ensures:
- **Single source of truth** for testing logic
- **Easy maintenance** (fix once, benefit everywhere)
- **Project flexibility** (custom configurations)
- **Gradual adoption** (coexistence with old commands)
- **Clear patterns** (consistent with existing workflows)

Next step: **Begin Phase 2** - Create the three canonical workflows.

---

**Document Status**: Complete and ready for implementation
**Last Updated**: 2025.10.11
**Version**: 1.0
